This **project** leverages a transformer model created by **LAION**, built upon the weights of Meta's recently introduced **LLaMA LLM**. Specifically, the model used is oasst-sft-6-llama-30b, with inference facilitated through _HuggingFace's HuggingChat API_. To optimize the chatbot's performance, we employ _Instruction-Prompt-Tuning (IPT)_, which provides a structured set of guidelines for the model's behavior. These instructions are applied at the beginning of each session to ensure the chatbot maintains a consistent tone and delivers accurate, relevant responses tailored to its role as a medical assistant. This approach shapes both the initial prompt and the responses generated, enhancing the chatbot's ability to assist users with healthcare-related inquiries effectively.

After considering the advantages and limitations of IPT or soft-tuning versus fine-tuning the model within the constrained timeline of the hackathon, we determined that IPT was a more practical choice. This decision balanced factors such as deployment complexity and inference costs, while still meeting the project's technical requirements.
